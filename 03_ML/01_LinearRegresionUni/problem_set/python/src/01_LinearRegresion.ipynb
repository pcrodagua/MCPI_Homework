{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regresion with One Variable (Lecture: 01)\n",
    "\n",
    "__Author__: Pablo César Rodríguez Aguayo\n",
    "\n",
    "__UI__: MCs in Information Processing\n",
    "\n",
    "__Subject__: Machine Learning\n",
    "\n",
    "__Date__: 06/02/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regresion\n",
    "\n",
    "\n",
    "## Model Representation\n",
    "The first goal of a machine learning algorithm is to create a _model*_ htat can be used to estimate _y_ based on _x_. The _hypothesis_, maps the _input values_ to the _output values_. By trainning a model, we can estimate for example the next future value based on the input values. This type of problems is called a _regression problem_, that given some input we obtain a continuous output.\n",
    "For this case, Linear regresion can be presented as:\n",
    "\n",
    "### $h_\\theta(x) = \\theta_0 + \\theta_1 x$\n",
    "\n",
    "Where theta($\\theta$) values area the parameters used.\n",
    "\n",
    "The goal of creating a model is to choose the parameters, so that $h(x)$ is close to the output variable for the trainning data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function (Mean Difference Squared)\n",
    "\n",
    "The problem in this machine learning problem is to find the proper values of $\\theta$ so they feed in the data correctly. So what do we do? First, we need a function that will minimize the parameters over our data. One function that we will be using is: __Mean Squared Error__(Mean Squared Deviation). This function will help us measureing the distance between the estimator(dataset) and the estimated values(prediction values).\n",
    "\n",
    "The original MSE function looks like this:\n",
    "\n",
    "### $MSE = \\frac{1}{n}\\sum _{i = 1}^{n}(Y_i - \\hat{Y_i})^2$\n",
    "\n",
    "To find out good values for the parameters, we want to minimize the difference between the calculated results and the actual results of our data. So we substract:\n",
    "\n",
    "### $h_\\theta(x^{(i)}) - y^i$\n",
    "\n",
    "Now we do this for all $i$ from 1 to $m$. Since we want to calculate the sum over the difference and then calculate the average by multiplying the sum by $\\frac{1}{n}$, getting as a result:\n",
    "\n",
    "### $J(\\theta_0, \\theta_1) = \\frac{1}{m}\\sum _{i = 1}^{m}(h_\\theta(x^{i}) - y^i)^2$\n",
    "\n",
    "So now, we will call $J(\\theta_0, \\theta_1)$ the cost function. In this function, we have all the right values to fit the model into our data, but now we have the problem of finding the right values. So how do we do this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "To find the proper values that fit our model we use the _Gradient Descent Algorithm_. The Gradient Descent is a general function for minimizing a function, in this case the Mean Square Error Cost Function.\n",
    "\n",
    "What Gradient Descent basically does is iterate over the theta values one by one until we arrive at a minimum or at least get close to it.\n",
    "\n",
    "So first, we initialize the parameters in 0, and start from there to search for the values.\n",
    "\n",
    "### $\\theta_j := \\theta_j - \\alpha\\frac{\\partial }{\\partial \\theta_i}J(\\theta_0, \\theta_1)$ (for j = 0 and j = 0)\n",
    "\n",
    "where $\\alpha$ is the _learning rate_ or how quickly the algorithm will move towards the minimum value. We have to consider that __If $\\alpha$ is too large, we can overshoot the learning.__\n",
    "\n",
    "![Gradient Descend Algorithm Visualization](img/gda.png)\n",
    "\n",
    "In conclusion, we have a hypothesis:\n",
    "\n",
    "##### $h_\\theta(x) = \\theta_0 + \\theta_1 x$\n",
    "\n",
    "that we want to fit in our trainning data so we use a Mean Squared Error function:\n",
    "\n",
    "#### $MSE = \\frac{1}{n}\\sum _{i = 1}^{n}(Y_i - \\hat{Y_i})^2$\n",
    "\n",
    "wich we can minimize using the Gradient Descend algorithm:\n",
    "#####  $\\theta_j := \\theta_j - \\alpha\\frac{\\partial }{\\partial \\theta_i}J(\\theta_0, \\theta_1)$ (for j = 0 and j = 0)\n",
    "\n",
    "Obtainning the first machine learning algorithm called: Linear Regression. Putting it together the partial derivate of the cost function:\n",
    "\n",
    "$\\frac{\\partial }{\\partial \\theta_i}$\n",
    "\n",
    "we obtain:\n",
    "\n",
    "#### repeat until convergence {\n",
    "####     $\\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum _{i = 1}^{m}(h_\\theta(x^{i}) - y^i)$\n",
    "####     $\\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum _{i = 1}^{m}(h_\\theta(x^{i}) - y^i)x^{(i)}$\n",
    "#### }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'terminaltables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b1be4c3dc212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mterminaltables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsciiTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolynomial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolynomial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpolyfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'terminaltables'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from terminaltables import AsciiTable\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Square Error Function\n",
    "def calculate_MSE(theta_0, theta_1, data_):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(data_)):\n",
    "        x = data_[i, 0]\n",
    "        y = data_[i, 1]\n",
    "        totalError += ((theta_0 + theta_1 * x) - y) ** 2\n",
    "    return totalError/float(len(data_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct: Simultaneous update J\n",
    "def correct_update(theta_0, theta_1, data_, alpha):\n",
    "    temp0 = 0\n",
    "    temp1 = 0\n",
    "    M = float(len(data_))\n",
    "    for i in range(0, int(M)):\n",
    "        x = data_[i, 0]\n",
    "        y = data_[i, 1]\n",
    "        temp0 += (theta_0 + (theta_1 * x) - y)\n",
    "        temp1 += (theta_0 + (theta_1 * x) - y) * x\n",
    "    new_theta_0 = theta_0 - (alpha * temp0)/M\n",
    "    new_theta_1 = theta_1 - (alpha * temp1)/M\n",
    "    return [new_theta_0, new_theta_1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Algorithm\n",
    "def calculate_GDA(data_, strt_theta_0, strt_theta_1, alpha, iterations):\n",
    "    theta_0 = strt_theta_0\n",
    "    theta_1 = strt_theta_1\n",
    "    for i in range(iterations):\n",
    "        theta_0, theta_1 = correct_update(theta_0, theta_1, np.array(data_), alpha)\n",
    "    return [theta_0, theta_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ebb013327609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "# main\n",
    "def main():\n",
    "    np.set_printoptions(precision=300)\n",
    "    data = np.genfromtxt(\"data.csv\", delimiter=\",\")\n",
    "    alpha = 0.01 # learning rate\n",
    "    theta_0 = 0 # interception with y\n",
    "    theta_1 = 0 # slope of the function\n",
    "    iterations = 1500\n",
    "    [theta_0, theta_1] = calculate_GDA(data, theta_0, theta_1, alpha, iterations)\n",
    "    error = calculate_MSE(theta_0, theta_1, data)\n",
    "    \n",
    "    table_data = [\n",
    "        ['Parameter', 'Value'],\n",
    "        ['Iterations', '{0}'.format(iterations)],\n",
    "        ['Theta 0', '{0}'.format(theta_0)],\n",
    "        ['Theta 1', '{0}'.format(theta_1)],\n",
    "        ['Error', '{0}'.format(error)]\n",
    "    ]\n",
    "    \n",
    "    table = AsciiTable(table_data)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['x', 'y'])\n",
    "    Y = theta_0 + theta_1 * df.x\n",
    "    \n",
    "    t0, t1 = polyfit(df.x, Y, 1)\n",
    "    plt.title('Failures per Decade in the State of Zacatecas')\n",
    "    plt.ylabel('Failures')\n",
    "    plt.xlabel('Years')\n",
    "    plt.plot(df.x, df.y, '.', label='h(x)={0} + {1}x'.format(theta_0, theta_1))\n",
    "    plt.plot(df.x, t0 + t1 * df.x, '-')\n",
    "    plt.show()\n",
    "    print(table.table)\n",
    "    \n",
    "    print(\"Prediction for 2050: {0}\".format(theta_0 + theta_1 * 50))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    if sys.path[0] == '':\n",
    "        del sys.path[0]\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
